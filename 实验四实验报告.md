# 实验四

#### 171098064 马小堤

### 1. Spark环境配置

实验所用系统为macOS 10.15.7。首先通过Homebrew下载安装scala。

```shell
brew install scala
```

`vim ~/.bash_profile`配置scala环境变量。

```shell
export SCALA_HOME=/usr/local/Cellar/scala/2.13.4/libexec
export PATH=$PATH:$SCALA_HOME/bin
```

执行 `scala -version`，验证scala安装成功。

<img src="/Users/sheddy_ma/Library/Application Support/typora-user-images/image-20201215140500984.png" alt="image-20201215140500984" style="zoom:50%;" />

通过Homebrew下载安装Spark。

```shell
brew install apache-spark
```

`vim ~/.bash_profile`配置spark环境变量。

```shell
export SPARK_HOME=/usr/local/Cellar/apache-spark/3.0.1/libexec
export PATH=$PATH:$SPARK_HOME/bin
```

进入spark安装路径下`/conf`文件夹，由于该目录下只有spark-env.sh.template和slaves.template文件，因此进行拷贝和重命名。

```shell
cp spark-env.sh.template spark-env.sh
cp slaves.template slaves
```

编辑spark-env.sh，添加如下内容：

```shell
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home
export SCALA_HOME=/usr/local/Cellar/scala/2.13.4/libexec
export SPARK_MASTER_HOST=localhost
export SPARK_MASTER_PORT=7077
```

在 `/sbin`目录下执行 `./start-all.sh`（避免和之前安装的hadoop产生冲突），随后运行 `jps`查看当前进程。

<img src="/Users/sheddy_ma/Library/Application Support/typora-user-images/image-20201215144334436.png" alt="image-20201215144334436" style="zoom:50%;" />

有Worker和Master节点，则spark启动成功。进入web界面 `locahost:8080`：

![image-20201215144630595](/Users/sheddy_ma/Library/Application Support/typora-user-images/image-20201215144630595.png)

则spark伪分布式配置成功。

### 2. 任务一：分别编写mapreduce和spark应用程序统计双十一最热门的商品和最受年轻人关注的商家

#### 2.1 MapReduce统计程序

创建名为Task1的java应用项目。

```shell
mvn archetype:generate "-DgroupId=t1" "-DartifactId=Task1" "-DarchetypeArtifactId=maven-archetype-quickstart" "-DinteractiveMode=false"  
```

与WordCount的思路类似，实现两个MapReduce job。

第一个job（GoodsMapper+GoodsReducer）实现最热门商品的统计。map函数中读取用户行为日志，通过设置每一行拆分出的字符串数组长度作为条件过滤掉含有缺失值的行，同时剔除action_type=0的仅单击数据，将（item_id, action_type）键值对传入Reducer进行统计。Reducer中通过cleanup()函数，将reduce()输出的结果降序排序，并输出前100名。

最开始没有考虑到用户行为日志中存在同一用户对同一商品反复加入购物车的情况，后续为了去重，在Mapper中定义了一个Set，将user_id+merchant_id作为一个字符串存入Set，仅当Set中不存在该匹配字符串时将键值对传入Reducer。

第二个job（MerchantsMapper+MerchantsReducer）实现最受年轻人关注商家的统计。Mapper中在setup()函数里读取用户画像数据，筛选出30岁以下的user_id，map函数中匹配包含这部分user_id的数据，将（merchant_id, action_type）键值对传入reducer进行统计。Reducer中同样通过cleanup()函数将结果排序并部分输出。

#### 2.2 Spark统计程序

同样利用maven新建一个项目，由于需要使用scala，因此在IDEA里配置project structure中的global libraries，加入scala jdk。此处需要注意的是scala的最新版本为2.13.4，然而maven repository中对应spark 3.0.1版本的scala为2.12版本，因此需要重新browse搜索2.12版本的scala jdk并配置到项目中，否则运行会报错。

将 `src/main`目录下的java文件夹重命名为scala，在package st1中新建一个scala class，名称为Count，类型为Object。将用户行为日志和用户信息读入为rdd，去除标题行并按“,”划分，筛选出time_stamp = "1111"的双十一操作数据，同时将两个rdd通过map操作转化为<key,value>形式，其中key为user_id，value为余下特征。

统计双十一最热门商品时，将user_log通过map转化为<item_id, action_type>格式，然后按照item_id进行reduce统计，倒序排列并输出前100项。统计最受年轻人欢迎的商家时，将user_info和user_log按照user_id进行join操作，filter过滤出age_range取1、2或3的行，将每行转化为<seller_id, action_type>的格式，然后按照seller_id进行reduce统计，同样倒序排列并输出前100项。

### 3. 任务二：编写Spark程序统计双十一购买了商品的男女比例，以及购买了商品的买家年龄段的比例

读取user_info和user_log数据，user_info中取user_id为key、age_range和gender为value，同时剔除年龄段或性别未知的数据；user_log中取user_id为key、action_type为value，筛选出双十一当日的数据及操作类型为购买的数据，并利用distinct进行去重。将两个rdd进行join操作，按照不同特征进行filter和count统计，最终得出计算结果：

<img src="/Users/sheddy_ma/Library/Application Support/typora-user-images/image-20201231232033725.png" alt="image-20201231232033725" style="zoom:50%;" />

### 4. 任务三：基于Hive或者Spark SQL查询双十一购买了了商品的男女比例，以及购买了商品的买家年龄段的比例

和Spark设计的思路类似，在利用SparkSQL读取数据时预先设置好schema（也能够加快读取的速度），从user_log中提取user_id和action_type字段，筛选双十一和购买数据，同时利用.dropDuplicates()函数进行去重处理，剔除所有含有空值的行。将两个dataframe通过user_id进行join操作，对于每一类经过groupBy分组并count()计数，最终得出计算结果：

<img src="/Users/sheddy_ma/Library/Application Support/typora-user-images/image-20210101153946047.png" alt="image-20210101153946047" style="zoom:45%;" />

### 5. 任务四：预测给定的商家中，哪些新消费者在未来会成为忠实客户，即需要预测这些新消费者在6个月内再次购买的概率。基于Spark MLlib编写程序预测回头客，评估实验结果的准确率

任务四采用python编写，利用pyspark.mllib完成机器学习流程。

原始数据文件中包含的特征信息有age_range和gender，但是通过绘制图像发现，训练集中的样本特征分布很不均衡，且与分类标签相关性较低。

<img src="/Users/sheddy_ma/Library/Application Support/typora-user-images/image-20201230131600162.png" alt="image-20201230131600162" style="zoom:30%;" />

<img src="/Users/sheddy_ma/Library/Application Support/typora-user-images/image-20201230131623503.png" alt="image-20201230131623503" style="zoom:40%;" />

因此在原始数据基础上再构建一系列新的特征。新的特征条目包括单一用户行为日志的总数、用户浏览的商品的数目、浏览的商品的种类的数目、用户浏览的天数、用户单击的次数、用户添加购物车的次数、用户购买的次数、用户收藏的次数，具体实现方法包含于ipynb文件中。构建完新的特征后，再次绘制此时的相关图。

<img src="/Users/sheddy_ma/Library/Application Support/typora-user-images/image-20201230131911569.png" alt="image-20201230131911569" style="zoom:45%;" />

虽然仍存在相关性不足的问题，但是已有部分改善，其余将在后续环节中寻求进一步改进。

对训练集和测试集进行描述性统计后发现，训练集和测试集用户的数量相当，约为26万条，而训练集中两类用户数量很不均衡，标签为0的有约25万条，标签为1的约1万6千条，这会严重影响分类的结果。尝试对训练集再次进行采样以平衡两类标签的样本，考虑上采样方法和下采样方法。

下采样从标签为0的训练数据集中随机抽取与少数类数量相同的子集，连接构成新的训练集。新的训练集共有31904条数据，其中两类标签的样本比例为1：1。

上采样采取SMOTE（Synthetic Minority Oversampling Technique），即合成少数类过采样技术，基本思想是对少数类样本进行分析并根据少数类样本人工合成新样本添加到数据集中。上采样后的训练集共有489824条数据，其中两类标签的样本比例为1：1。

机器学习分类器方面选取逻辑回归、支持向量机、随机森林、梯度提升四种分类器进行测试。首先使用下采样生成的新训练集。根据天池上提交结果的正确率检测，最大深度为10、决策树数量为100的随机森林分类器效果最好。逻辑回归和支持向量机的训练速度最快，而梯度提升的训练速度最慢。在随机森林分类器中，最大深度参数的调整带来的正确率提升效果优于决策树数量调整带来的正确率提升。

使用上采样生成的新训练集进行训练后，发现分类正确率最高仅为0.553，正确率反而下降了。查询认为，上采样可能出现在近邻选择时存在一定的盲目性的问题。在算法执行过程中,需要确定K值，即选择多少个近邻样本。K值的下限是M值（M值为从K个近邻中随机挑选出的近邻样本的个数,且有M< K），M的大小可以根据负类样本数量、正类样本数量和数据集最后需要达到的平衡率决定。但K值的上限没有办法确定，只能根据具体的数据集去反复测试。因此如何确定K值，才能使算法达到最优是未知的。此外，上采样无法克服非平衡数据集的数据分布问题，容易产生分布边缘化问题。由于负类样本的分布决定了其可选择的近邻,如果一个负类样本处在负类样本集的分布边缘，则由此负类样本和相邻样本产生的“人造”样本也会处在这个边缘，且会越来越边缘化，从而模糊了正类样本和负类样本的边界，而且使边界变得越来越模糊。这种边界模糊性，虽然使数据集的平衡性得到了改善，但加大了分类算法进行分类的难度。

对于分类的进一步优化思考，首先过程中所涉及的各个参数都存在优化空间，可以通过多次重复尝试找到最佳参数，会提升分类效果；此外，由于训练集中两个label数据量相差过于悬殊，参考网络搜索建议，可以考虑采用异常值监测而非传统二分类方法进行0/1类别划分。